

## ğŸ“– Project Overview
This project implements a complete **Retrieval-Augmented Generation (RAG)** pipeline for answering medical-related questions. The system retrieves relevant medical context from a vector database and generates accurate, context-grounded responses using a **4-bit quantized Large Language Model (LLM)** powered by **Unsloth**.

The project is optimized for **low-GPU environments** and runs efficiently on **Google Colab (T4 GPU)** with reduced memory usage.

> âš ï¸ **Legal Disclaimer**  
> This project is strictly for **educational and research purposes only**.  
> It does **not provide medical advice**, diagnosis, or treatment.

---

## ğŸ¯ Objectives
- Implement a complete RAG pipeline (Retrieve â†’ Augment â†’ Generate)
- Use **Unsloth dynamic 4-bit quantization** for memory efficiency
- Store and retrieve documents using a **vector database**
- Reduce hallucinations by grounding responses in retrieved context
- Ensure compatibility with free-tier GPU environments

---

## ğŸ—ï¸ System Architecture
```

User Query
â†“
Sentence Embedding
â†“
Vector Database (ChromaDB)
â†“
Relevant Medical Context
â†“
Prompt Augmentation
â†“
4-bit Quantized LLM (Unsloth)
â†“
Context-Grounded Answer

```

---

## âœ¨ Key Features
- âœ… Complete Retrieval-Augmented Generation (RAG) pipeline  
- âœ… Unsloth-powered **4-bit quantized LLM**  
- âœ… Semantic search using **ChromaDB**  
- âœ… Medical domain document processing  
- âœ… Reduced GPU memory usage  
- âœ… Modular and easy-to-extend design  

---

## ğŸ“ Project Structure
```

medical-rag-unsloth/
â”œâ”€â”€ README.md
â”œâ”€â”€ rag_pipeline.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ medical_rag_colab.ipynb
â””â”€â”€ .gitignore

````

---

## ğŸ”§ Technical Implementation

### 1ï¸âƒ£ Model Loading (Unsloth 4-bit Quantization)
The language model is loaded using Unsloth with dynamic 4-bit quantization to significantly reduce memory usage.

```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/tinyllama-bnb-4bit",
    max_seq_length=512,
    load_in_4bit=True
)
````

---

### 2ï¸âƒ£ Vector Database (ChromaDB)

Medical documents are embedded using Sentence Transformers and stored in an **in-memory vector database** for semantic retrieval.

```python
import chromadb

client = chromadb.Client()
collection = client.create_collection("medical_knowledge")
```

---

### 3ï¸âƒ£ Retrieval-Augmented Generation Flow

1. Convert user query into embeddings
2. Retrieve top relevant documents from vector database
3. Combine retrieved context with the user question
4. Generate response using the LLM

```text
Context + Question â†’ Prompt â†’ LLM â†’ Answer
```

---

## ğŸš€ How to Run the Project

### â–¶ï¸ Google Colab (Recommended)

```bash
git clone https://github.com/YOUR-USERNAME/medical-rag-unsloth.git
cd medical-rag-unsloth
pip install -r requirements.txt
python rag_pipeline.py
```

### â–¶ï¸ Local Machine

```bash
pip install -r requirements.txt
python rag_pipeline.py
```

---

## ğŸ§ª Example Usage

```python
from rag_pipeline import MedicalRAG

rag = MedicalRAG()

rag.add_documents([
    "Diabetes treatment includes Metformin 500mg twice daily.",
    "Hypertension target blood pressure is below 130/80 mmHg."
])

response = rag.query("What is the treatment for diabetes?")
print(response)
```

---

## ğŸ“Š Performance & Evaluation

* **Model Size:** ~1GB (4-bit quantized)
* **VRAM Usage:** ~5â€“7GB
* **Retrieval Time:** <0.5 seconds (small datasets)
* **Evaluation Type:** Qualitative evaluation based on context relevance

> Note: RAG systems are evaluated qualitatively unless benchmark datasets and metrics are explicitly defined.

---

## ğŸ› ï¸ Technologies Used

* Python
* Unsloth
* Transformers
* PyTorch
* ChromaDB
* Sentence-Transformers
* NumPy

---

## ğŸ“ Learning Outcomes

* Understanding Retrieval-Augmented Generation (RAG)
* Implementing vector-based semantic search
* Deploying memory-efficient LLMs using 4-bit quantization
* Handling medical NLP responsibly
* Working with GPU constraints

